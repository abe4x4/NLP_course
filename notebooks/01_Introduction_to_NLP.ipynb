{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction to NLP\n",
    "\n",
    "Welcome to the first lesson of our NLP course! In this section, we will cover the fundamental concepts of Natural Language Processing and perform some basic text preprocessing tasks.\n",
    "\n",
    "**Our goals for this section are:**\n",
    "1. Understand what NLP is.\n",
    "2. Learn how to tokenize text.\n",
    "3. Understand the importance of removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A Quick Refresher\n",
    "\n",
    "As we discussed in the `getting_started_with_nlp.md` guide, NLP is all about making computers understand and process human language. The first step in doing this is breaking down text into a more manageable form. Let's start with a simple sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"NLP is fascinating! It's a field of AI that has seen rapid growth in recent years.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into individual words or sentences, called **tokens**. This is a crucial first step for many NLP tasks.\n",
    "\n",
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['NLP', 'is', 'fascinating', '!', 'It', \"'s\", 'a', 'field', 'of', 'AI', 'that', 'has', 'seen', 'rapid', 'growth', 'in', 'recent', 'years', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Word Tokenization\n",
    "tokens = nltk.word_tokenize(sample_text)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy\n",
    "\n",
    "spaCy is another powerful NLP library. It's known for its speed and efficiency. Let's see how to tokenize with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Tokens: ['NLP', 'is', 'fascinating', '!', 'It', \"'s\", 'a', 'field', 'of', 'AI', 'that', 'has', 'seen', 'rapid', 'growth', 'in', 'recent', 'years', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "# Get tokens\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"spaCy Tokens:\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stop Words Removal\n",
    "\n",
    "Stop words are common words (like 'is', 'a', 'the') that often don't carry significant meaning. Removing them can help us focus on the more important words in the text.\n",
    "\n",
    "### Stop Words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after stop word removal: ['NLP', 'fascinating', '!', \"'s\", 'field', 'AI', 'seen', 'rapid', 'growth', 'recent', 'years', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Tokens after stop word removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy tokens after stop word removal: ['NLP', 'fascinating', '!', 'field', 'AI', 'seen', 'rapid', 'growth', 'recent', 'years', '.']\n"
     ]
    }
   ],
   "source": [
    "spacy_filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "print(\"spaCy tokens after stop word removal:\", spacy_filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now it's your turn!\n",
    "1. Create a new text variable with a sentence of your choice.\n",
    "2. Tokenize the text using either NLTK or spaCy.\n",
    "3. Remove the stop words from your tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  The quick brown fox jumps over the lazy dog. This is a classic sentence used for typography.\n",
      "Filtered Words:  ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'classic', 'sentence', 'typography']\n"
     ]
    }
   ],
   "source": [
    " # 1. Create a new text variable with a sentence of your choice.\n",
    "new_sentence = \"The quick brown fox jumps over the lazy dog. This is a classic sentence used for typography.\"\n",
    "      \n",
    "# We can reuse the 'nlp' object that was created in a previous cell\n",
    "doc = nlp(new_sentence)\n",
    "\n",
    "# 2. Tokenize the text and 3. Remove stop words and punctuation.\n",
    "# spaCy makes this easy. We loop through the 'doc' object and check if a token is a stop word or punctuation.\n",
    "\n",
    "filtered_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Print the final list of important words\n",
    "print(\"Original Sentence: \", new_sentence)\n",
    "print(\"Filtered Words: \", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Section 1\n",
    "\n",
    "Congratulations on completing the first section! You've learned how to perform two fundamental NLP tasks: tokenization and stop word removal.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Save this notebook.\n",
    "2. Commit your changes to Git with the message 'Complete Section 1'.\n",
    "3. When you're ready, ask me to proceed to **Section 2: Text Preprocessing**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
