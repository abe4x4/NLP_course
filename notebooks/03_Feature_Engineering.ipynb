
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers feature engineering techniques for NLP, including Bag-of-Words, TF-IDF, and Word Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag-of-Words (BoW) model is a simple representation of text that describes the occurrence of words within a document. It involves two parts: a vocabulary of known words and a measure of the presence of known words.\n\nImagine you have a collection of documents. The BoW model would represent each document as a vector, where each element in the vector corresponds to a word in the vocabulary. The value of each element could be the count of the word in the document (or a binary value indicating whether the word is present or not)."
   ],
  "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n\n# Sample documents\ndocuments = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?'\n]\n\n# Create a CountVectorizer object\nvectorizer = CountVectorizer()\n\n# Fit and transform the documents\nX = vectorizer.fit_transform(documents)\n\n# Get the feature names (vocabulary)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Print the vocabulary\nprint(\"Vocabulary:\", feature_names)\n\n# Print the BoW representation of the documents\nprint(\"BoW representation:\")\nprint(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF, which stands for Term Frequency-Inverse Document Frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n\n**Term Frequency (TF):** The number of times a word appears in a document divided by the total number of words in that document.\n\n**Inverse Document Frequency (IDF):** The logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Sample documents\ndocuments = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?'\n]\n\n# Create a TfidfVectorizer object\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the documents\nX = vectorizer.fit_transform(documents)\n\n# Get the feature names (vocabulary)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Print the vocabulary\nprint(\"Vocabulary:\", feature_names)\n\n# Print the TF-IDF representation of the documents\nprint(\"TF-IDF representation:\")\nprint(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are a type of word representation that allows words with similar meanings to have a similar representation. They are dense vector representations of words, where each word is represented by a vector of real numbers.\n\nUnlike Bag-of-Words and TF-IDF, which are sparse representations, word embeddings capture the semantic relationships between words. This is because the vectors are learned from the context in which words appear.\n\nThere are several popular pre-trained word embedding models, such as Word2Vec, GloVe, and FastText. These models have been trained on massive amounts of text data and can be used to initialize the weights of a neural network or to compute word similarities."
   ],
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: To run this code, you will need to download a pre-trained GloVe model.\n# You can download it from here: https://nlp.stanford.edu/projects/glove/\n# We will use the 'glove.6B.100d.txt' file.\n\nimport numpy as np\n\ndef load_glove_model(glove_file):\n    print(\"Loading GloVe Model\")\n    model = {}\n    with open(glove_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            split_line = line.split()\n            word = split_line[0]\n            embedding = np.array([float(val) for val in split_line[1:]])\n            model[word] = embedding\n    print(f\"Done. {len(model)} words loaded!\")\n    return model\n\n# Path to the GloVe file\nglove_file = 'path/to/glove.6B.100d.txt'\n\n# Load the GloVe model\n# model = load_glove_model(glove_file)\n\n# # Example: Get the vector for a word\n# print(\"Vector for 'king':\", model['king'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
